<!DOCTYPE html>
<html lang="en-us">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='Entropy 1. Shannon information
\[ I = -\log_2{p} \]
 $ p $ is the probability of the event Event with smaller probability contains more information. Logrithm base is 2 beacause in information technology 1 bit represents &amp;quot;0&amp;quot; or &amp;quot;1&amp;quot;.  It can also be regarded as how many bits we need to represent a random variable
\[ \#bits = \log_2{1\over{p}} \]
For example, when one variable has 8 possibilities. Each of them has a probability of 1/8.'><title>Decision Tree and k-Nearest Neighbors Learning</title>

<link rel='canonical' href='https://yuukidach.github.io/p/ml/decision-tree-and-knn-learning/'>

<link rel="stylesheet" href="/scss/style.min.css"><meta property='og:title' content='Decision Tree and k-Nearest Neighbors Learning'>
<meta property='og:description' content='Entropy 1. Shannon information
\[ I = -\log_2{p} \]
 $ p $ is the probability of the event Event with smaller probability contains more information. Logrithm base is 2 beacause in information technology 1 bit represents &amp;quot;0&amp;quot; or &amp;quot;1&amp;quot;.  It can also be regarded as how many bits we need to represent a random variable
\[ \#bits = \log_2{1\over{p}} \]
For example, when one variable has 8 possibilities. Each of them has a probability of 1/8.'>
<meta property='og:url' content='https://yuukidach.github.io/p/ml/decision-tree-and-knn-learning/'>
<meta property='og:site_name' content='Dash&#39;s Blog'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:published_time' content='2020-11-18T00:00:00&#43;00:00'/><meta property='article:modified_time' content='2020-11-18T00:00:00&#43;00:00'/>
<meta name="twitter:title" content="Decision Tree and k-Nearest Neighbors Learning">
<meta name="twitter:description" content="Entropy 1. Shannon information
\[ I = -\log_2{p} \]
 $ p $ is the probability of the event Event with smaller probability contains more information. Logrithm base is 2 beacause in information technology 1 bit represents &amp;quot;0&amp;quot; or &amp;quot;1&amp;quot;.  It can also be regarded as how many bits we need to represent a random variable
\[ \#bits = \log_2{1\over{p}} \]
For example, when one variable has 8 possibilities. Each of them has a probability of 1/8.">
    </head>
    <body class="">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "dark");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.body.dataset.scheme = 'dark';
        } else {
            document.body.dataset.scheme = 'light';
        }
    })();
</script><div class="container main-container flex on-phone--column extended article-page with-toolbar">
            <aside class="sidebar left-sidebar sticky">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header class="site-info">
        
            <figure class="site-avatar">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hub6321020dedfcea2dceafae09ca2caaa_21891_300x0_resize_q75_box.jpeg" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                

                
                    <span class="emoji">üç•</span>
                
            </figure>
        
        <h1 class="site-name"><a href="https://yuukidach.github.io/">Dash&#39;s Blog</a></h1>
        <h2 class="site-description">A long long way to find myself.</h2>
    </header>

    <ol class="menu" id="main-menu">
        
        
        

        <li >
            <a href='/'>
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>Home</span>
            </a>
        </li>
        
        

        <li >
            <a href='/about'>
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>About</span>
            </a>
        </li>
        
        

        <li >
            <a href='/archives'>
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        

        <li >
            <a href='/search'>
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        

        
            <li id="dark-mode-toggle">
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                <span>Dark Mode</span>
            </li>
        
    </ol>
</aside>

            <main class="main full-width">
    <div id="article-toolbar">
        <a href="https://yuukidach.github.io/" class="back-home">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="15 6 9 12 15 18" />
</svg>



            <span>Back</span>
        </a>
    </div>

    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/machine-learning/" >
                Machine Learning
            </a>
        
    </header>
    

    <h2 class="article-title">
        <a href="/p/ml/decision-tree-and-knn-learning/">Decision Tree and k-Nearest Neighbors Learning</a>
    </h2>

    <footer class="article-time">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



        <time class="article-time--published">Nov 18, 2020</time>
    </footer></div>
</header>

    <section class="article-content">
    <h2 id="entropy">Entropy</h2>

<p>1. Shannon information</p>

<p><span  class="math">\[
I = -\log_2{p}
\]</span></p>

<ul>
<li>$ p $ is the probability of the event</li>
<li>Event with smaller probability contains more information.</li>
<li>Logrithm base is 2 beacause in information technology 1 bit represents &quot;0&quot; or &quot;1&quot;.</li>
</ul>

<p>It can also be regarded as how many bits we need to represent a random variable</p>

<p><span  class="math">\[
\#bits = \log_2{1\over{p}}
\]</span></p>

<p>For example, when one variable has 8 possibilities. Each of them has a probability of 1/8. Then we need $ \log_2{8} = 3 $ bits to represent the variable.</p>

<p>2. Shannon entropy</p>

<p><span  class="math">\[
H = - \sum_{i=1}^{n}{p_i\log_{2}{p}_i}
\]</span></p>

<ul>
<li>$ H $ is sum of all possible events</li>
<li><code>H = 1</code> means completly uncertain about the result. <code>H = 0</code> means the result is known.</li>
</ul>

<p>For example, if we throw a coin, it will have 2 results, both probabilities is 0.5.</p>

<p><span  class="math">\[
H = -0.5 \times \log_2{0.5} - 0.5 \times \log_2{0.5} = 1
\]</span></p>

<p>The result is totally uncertain.</p>

<p>3. Information gain</p>

<p><span  class="math">\[
IG(T, a) = H(T) - H(T|a) \\
H (T|a) = -\sum_{x \in {a}, y \in{T}}{p(x, y)\log{p(x, y) \over {p(x)}}}
\]</span></p>

<ul>
<li>$ H(T|a) $ is the <a href="https://en.wikipedia.org/wiki/Conditional_entropy">conditional entropy</a> of T given the value of attribute a.</li>
</ul>

<h2 id="id3-iterative-dichotomiser-3-algorithm">ID3 (Iterative Dichotomiser 3) Algorithm</h2>

<p>ID3 is an algorithm invented by Ross Quinlan used to generate a decision tree from a dataset.</p>

<p>It can be used for dataset with categorical features like:</p>

<table>
<thead>
<tr>
<th align="center">Day</th>
<th align="center">Outlook</th>
<th align="center">Temperature</th>
<th align="center">Humidity</th>
<th align="center">Wind</th>
<th align="center">Play ball</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">D1</td>
<td align="center">Sunny</td>
<td align="center">Hot</td>
<td align="center">High</td>
<td align="center">Weak</td>
<td align="center">No (-)</td>
</tr>

<tr>
<td align="center">D2</td>
<td align="center">Sunny</td>
<td align="center">Hot</td>
<td align="center">High</td>
<td align="center">Strong</td>
<td align="center">No (-)</td>
</tr>

<tr>
<td align="center">D3</td>
<td align="center">Overcast</td>
<td align="center">Hot</td>
<td align="center">High</td>
<td align="center">Weak</td>
<td align="center">Yes (+)</td>
</tr>

<tr>
<td align="center">D4</td>
<td align="center">Rain</td>
<td align="center">Mild</td>
<td align="center">High</td>
<td align="center">Weak</td>
<td align="center">Yes (+)</td>
</tr>

<tr>
<td align="center">D5</td>
<td align="center">Rain</td>
<td align="center">Cool</td>
<td align="center">Normal</td>
<td align="center">Weak</td>
<td align="center">Yes (+)</td>
</tr>

<tr>
<td align="center">D6</td>
<td align="center">Rain</td>
<td align="center">Cool</td>
<td align="center">Normal</td>
<td align="center">Strong</td>
<td align="center">No (-)</td>
</tr>

<tr>
<td align="center">D7</td>
<td align="center">Overcast</td>
<td align="center">Cool</td>
<td align="center">Normal</td>
<td align="center">Strong</td>
<td align="center">Yes (+)</td>
</tr>

<tr>
<td align="center">D8</td>
<td align="center">Sunny</td>
<td align="center">Mild</td>
<td align="center">High</td>
<td align="center">Weak</td>
<td align="center">No (-)</td>
</tr>
</tbody>
</table>

<p>Pseudocode:</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">ID3 <span class="o">(</span>Examples, Target_Attribute, Attributes<span class="o">)</span>
    Create a root node <span class="k">for</span> the tree
    If all examples are positive, Return the single-node tree Root, with <span class="nv">label</span> <span class="o">=</span> +.
    If all examples are negative, Return the single-node tree Root, with <span class="nv">label</span> <span class="o">=</span> -.
    If number of predicting attributes is empty, <span class="k">then</span> Return the single node tree Root,
    with <span class="nv">label</span> <span class="o">=</span> most common value of the target attribute in the examples.
    Otherwise Begin
        A ‚Üê The Attribute that best classifies examples.
        Decision Tree attribute <span class="k">for</span> <span class="nv">Root</span> <span class="o">=</span> A.
        For each possible value, vi, of A,
            Add a new tree branch below Root, corresponding to the <span class="nb">test</span> <span class="nv">A</span> <span class="o">=</span> vi.
            Let Examples<span class="o">(</span>vi<span class="o">)</span> be the subset of examples that have the value vi <span class="k">for</span> A
            If Examples<span class="o">(</span>vi<span class="o">)</span> is empty
                Then below this new branch add a leaf node with <span class="nv">label</span> <span class="o">=</span> most common target value in the examples
            Else below this new branch add the subtree ID3 <span class="o">(</span>Examples<span class="o">(</span>vi<span class="o">)</span>, Target_Attribute, Attributes ‚Äì <span class="o">{</span>A<span class="o">})</span>
    End
    Return Root</code></pre></div>
<p>ID3 does not guarantee an optimal solution. It can converge upon local optima. It uses a greedy strategy by selecting the locally best attribute to split the dataset on each iteration.</p>

<h2 id="c45-algorithm">C4.5 Algorithm</h2>

<p>It can be used for data with continuous features.</p>

<p>Procedure:</p>

<ol>
<li>Sort the data records by the attribute values</li>
<li>Calculate the partition point for 2 consecutive records by $ (v<em>i + v</em>{i+1})/2 $</li>
<li>Partition the records into 2 sets by that partition point</li>
<li>Calculate the entropy reduction (information gain) of the resulting partitions</li>
<li>If all partition points are calculated, choose the point that yields the highest entropy reduction. Otherwise, ad i by 1 and go back to <code>step 2</code></li>
</ol>

<p>The whole process is nearly the same as ID3 algorithm, except for continuous feature, we need to calculate the partition point. But in ID3 algorithm, we can directly use the categories to split records.</p>

<h2 id="knearest-neighbors-algorithm">k-Nearest Neighbors Algorithm</h2>

<p>Distance between instance i and j</p>

<p><span  class="math">\[
d(x^{(i)}, x^{(j)}) = \sqrt{\sum_{r=1}^{n}{(f_r(x^{i})-f_r(x^{(j)}))^2}}
\]</span></p>

<p>$f_r(x)$ is the feature value of instance $ x $.</p>

<p>To predict a new instance $ x^{(q)} $:</p>

<p>1. Continues value</p>

<p><span  class="math">\[
\hat{f} \gets \frac{1}{k} \sum_{i=1}^{k}f(x^{(ki)})
\]</span></p>

<p>2. Discrete values</p>

<p><span  class="math">\[
\hat{f} \gets \text{argmax}_{v \in V} \sum_{i=1}^{k} I(f(x^{(i)}) == v)
\]</span></p>

<p>We can also use distance weighted nearest neighbor algorithm:</p>

<p><span  class="math">\[
\hat{f} \gets \frac{\sum_{i=1}^{k}{w_if(x^{(i)})}}{\sum_{i=1}^{k}{w_i}}, \ \text {where} \ w_i = \frac{1}{d(x^{(q)}, x^{(i)})^2}
\]</span></p>

<h2 id="evaluation-classification">Evaluation Classification</h2>

<p>1. Evaluation Metrics</p>

<p><figure><img src="evaluation_matrix.png" alt="Evaluation Metrics"></figure></p>

<p><span  class="math">\[
Accuracy = \frac{a+d}{a+b+c+d} \\
Precision = \frac{d}{b+d} \\
Sensitivity = Recall = \frac{d}{c+d} \\
Specificity = \frac{a}{a+b} \\
(False\ Positive\ Rate = 1 - Specificity)
\]</span></p>

<p>2. Area Under ROC Curve (AUC)</p>

<p><figure><img src="auc.png" alt="AUC"></figure></p>

<ul>
<li>Cutoff is the threshold a classification model uses to split between 2 classes.</li>
<li>As the cutoff decreases, more and more cases are classified as 1; hence, the sensitivity increases and speificity decreases.</li>
<li>As the ROC curvebows above the diagonal, the predictive power increases. Curve 1 is better than curve 2</li>
</ul>

<h2 id="issues-in-decision-tree-learning">Issues in Decision Tree Learning</h2>

<p>1. Features with many unique values</p>

<p><strong>Problem:</strong> <em>Gain</em> tends to select features with many unique values to clssify instance.</p>

<p><strong>Solution:</strong> Adjust <em>Gain</em> to <em>GainRatio</em></p>

<p><span  class="math">\[
SplitInfomation(S,A) \equiv -\sum_{i=1}^{c}\frac{|S_i|}{|S|}\log_2{\frac{|S_i|}{|S|}} \\
GainRatio(S,A) = \frac{Gain(S,A)}{SplitInfomation(S,A)} 
\]</span></p>

<p>$ S $ is the set of all records in a prarent node. $ S_i $ is a subset of records that have feature $ A_i $</p>

<p>When should Gain Ratio be used in place of Gain?</p>

<ul>
<li>Compute both Gain Ratio and Gain for each feature</li>
<li>Use Gain Ratio only for features with above-average Gain</li>
</ul>

<p>2. Overfitting</p>

<p>Ways to avoid overfitting:</p>

<ul>
<li>Stop growing tree when data split is not statistically significant.</li>
<li>Grow full tree then post-prune.</li>
</ul>

<p>Selecting best tree:</p>

<ul>
<li>Measure performance over taining data</li>
<li>Measure performance over seperate validation datases</li>
<li>Add penalty against complexity</li>
</ul>

<p>3. Unknown feature values</p>

<p>Strategies to impute missing values of feature A:</p>

<ul>
<li>Use most common value of A in all instances having missing value for A</li>
<li>Within each group of instances having same target value, assign most common value of A to instances</li>
</ul>

</section>


    <footer class="article-footer">
    

    </footer>

    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
    integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
    integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
    crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
    integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="StackLaTeX()"></script>

<script>
    function StackLaTeX() {
        renderMathInElement(document.querySelector(`.article-content`), {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ]
        });
    }
</script>
    
</article>

    <aside class="related-contents--wrapper">
    
    
</aside>


    
        
    <div class="disqus-container">
    <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "stack" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

<style>
    .disqus-container {
        background-color: var(--card-background);
        border-radius: var(--card-border-radius);
        box-shadow: var(--shadow-l1);
        padding: var(--card-padding);
    }
</style>

<script>
    window.addEventListener('onColorSchemeChange', (e) => {
        if (DISQUS) {
            DISQUS.reset({
                reload: true
            });
        }
    })
</script>

    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2016 - 
        
        2021 Dash&#39;s Blog
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        
    </section>
</footer>
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css">
            </main>
        </div>
        <script src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js"
    integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g=" crossorigin="anonymous"></script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
